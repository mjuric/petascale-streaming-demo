{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dirac.png\" width=\"700\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Alerts (with Genesis)\n",
    "\n",
    "By [@mjuric](http://github.com/mjuric), available at https://github.com/mjuric/petascale-streaming-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Alert Streaming and When to Use It\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic survey science is not performed in real time: the analysis usually usually lags the data collection and processing. Typically, a researcher waits (days to weeks) for a reasonably sized batch of new data to be accumulated, and then performs the analysis (and frequently manually).\n",
    "\n",
    "This does not work well in cases where the object of interest may change on short timescales and needs to be followed up rapidly. An example may be a short-timescale transient, or a potentially hazardous asteroid undergoing in an Earth flyby. For this use case, the researcher would prefer to be analyzing the data as they come in, with minimal latency (on order of seconds to minutes) between data collection and discovery/characterization.\n",
    "\n",
    "This is the problem alert streaming looks to solve: to **enable near real-time transmission of alerts to (and measurements of) objects whose properties have changed**. The key differences between the offline and streaming-driven research:\n",
    "1. Response time on order of seconds\n",
    "1. Fully automated, machine-driven, analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Challenges (Desirables)\n",
    "\n",
    "* Minimal latency (< seconds)\n",
    "* Robustness to client, server, and transport failiures (no data loss)\n",
    "* Ease of use (simple end-user interfaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology: Apache Kafka\n",
    "\n",
    "> Apache Kafka is a community distributed event streaming platform capable of handling trillions of events a day.\n",
    "> -- https://www.confluent.io/what-is-apache-kafka/\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> What is Kafka?\n",
    "> Kafka is a **distributed messaging system** providing **fast**, **highly scalable** and **redundant** messaging through a **pub-sub model**. Kafka’s distributed design gives it several advantages. First, Kafka allows a large number of permanent or ad-hoc consumers. Second, Kafka is highly available and resilient to node failures and supports automatic recovery. In real world data systems, these characteristics make Kafka an ideal fit for communication and integration between components of large scale data systems.\n",
    "> -- https://sookocheff.com/post/kafka/kafka-in-a-nutshell/\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> ![Kafka Cluster](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Overview_of_Apache_Kafka.svg/2560px-Overview_of_Apache_Kafka.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo setup: (Kafka Cluster + JupyterHub on Kubernetes) on Digital Ocean\n",
    "\n",
    "The demo runs on VMs on [Digital Ocean](http://digitalocean.com):\n",
    "* **Kafka broker cluster**: 8 x 6-core machine w. 16GB of RAM (\"standard\" [Droplet](https://www.digitalocean.com/pricing/) type)\n",
    "* **JupyterHub**: A 4-core machine w. 8GB of RAM **per user** (\"standard\" [Droplet](https://www.digitalocean.com/pricing/) type)\n",
    "\n",
    "Within the cluster, we've set up:\n",
    "1. A small static topic with only 100 alerts (named `small`)\n",
    "2. A medium-sized topic with 20,000 alerts (named `medium`)\n",
    "3. A topic with continuously injected alerts at LSST scale (named `lsst`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genesis Broker Access Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka comes with [performant Python libraries](https://docs.confluent.io/current/clients/confluent-kafka-python/) that largely follow the API and structure of their native (Java) client libraries. Unfortunately, that means they're not as \"Pythonic\" as they could be.\n",
    "\n",
    "Included with this demo is an early version of `genesis.streaming`, a client library for robust and scalable access to alert streams. Genesis aims to make it easy for an astronomer with some knowledge of Python to consume and filter alert stream.\n",
    "\n",
    "It also largely abstracts away both the underlying transport protocol and alert serialization: to the user, alerts are simple Python `dict`s, delivered through familiar `generator`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genesis.streaming as gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute the cell below, it will hang forever... (tip: click the stop ◾️button in your Jupyter to interrupt it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(\"kafka://genesis.alerts.wtf/small\") as stream:\n",
    "    for idx, alert in stream:\n",
    "        print(\"Candidate ID:\", alert['candid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? Genesis (actually, Kafka) remembers what was the last alert you received from any topic (the ***offset*** of the last received alert), and will only send you new alerts. This is desired behavior -- if you weren't connected immediately when the night started (or got temporarily disconnected), you may want to catch up.\n",
    "\n",
    "But what if this is your first time you connected to the stream? If so, Genesis will default to waiting for *new* packets, and not sending you anything it may already have. This is a safe default: e.g., if the first time you connect to the LSST stream is one year into operations, you don't want to be sent a year's worth of alerts! And in our case, since I'm not injecting any new alerts, it will wait indefinitely...\n",
    "\n",
    "Below is a visualization of [Kafka topics and offsets](https://kafka.apache.org/documentation/#intro_topics):\n",
    "![Kafka Offsets](https://kafka.apache.org/23/images/log_consumer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change this default, and have it send us everything it has. We'll also turn on a nice progress bar, and we will tell it to stop if it doesn't receive an alert in a 10 second interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(\"kafka://genesis.alerts.wtf/small\", start_at=\"earliest\") as stream:\n",
    "    for idx, alert in stream(timeout=10, progress=True):\n",
    "        print(\"[%d] Candidate ID: %s\" % (idx, alert['candid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice a slight pause before the streaming starts: this is Kafka establishing connections to the broker cluster. Once the connections are established, the alerts start streaming quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than printing the alerts, let's just store their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(\"kafka://genesis.alerts.wtf/small\", start_at=\"earliest\") as stream:\n",
    "    alerts = [ alert['candid'] for _, alert in stream(timeout=10, progress=True) ]\n",
    "\n",
    "alerts.sort()\n",
    "\n",
    "print(f\"Read {len(alerts)} alerts.\")\n",
    "print(\"First few candidate IDs:\", alerts[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remembering \"offsets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka remembers the `offset` of your last transmitted alert; next time you connect, it will continue streaming from that offset.\n",
    "\n",
    "For this to work, we need to connect to a stream with a \"consumer ID\" -- a name uniquely identifying you. Kafka will associate the offset to the consumer ID; the next time the same consumer ID connects, it will continue streaming the offset associated to it.\n",
    "\n",
    "Let's generate a random consumer ID for your demo session, and add it to the stream URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "\n",
    "id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))\n",
    "broker_url = \"kafka://{}@genesis.alerts.wtf/small\".format(id)\n",
    "\n",
    "print(\"Consumer ID:\", id)\n",
    "print(\"Broker URL: \", broker_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in a few alerts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(broker_url, start_at=\"earliest\") as stream:\n",
    "    alerts1 = [ alert['candid'] for _, alert in stream(limit=25, progress=True) ]\n",
    "    \n",
    "    # ... now do some work with the alert ...\n",
    "    \n",
    "    # stream.commit()\n",
    "\n",
    "alerts1.sort()\n",
    "\n",
    "print(f\"Read {len(alerts1)} alerts.\")\n",
    "print(\"First few candidate IDs:\", alerts1[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(broker_url, start_at=\"earliest\") as stream:\n",
    "    alerts2 = [ alert['candid'] for _, alert in stream(timeout=10, progress=True) ]\n",
    "\n",
    "    # ... now do some work with the alert ...\n",
    "    \n",
    "    # stream.commit()\n",
    "\n",
    "alerts2.sort()\n",
    "\n",
    "print(f\"Read {len(alerts2)} alerts.\")\n",
    "print(\"First few candidate IDs:\", alerts2[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't it continue from where we left off? Because the broker needs to be _explicitly_ told to commit the offset. It doesn't do so automatically to prevent data loss. \n",
    "\n",
    "To illustrate: if we committed the offset as soon as the alert is returned to you, and the code in _\"... now do some work with the alert ...\"_ section above crashes before acting on the alerts, the next time you connect to the broker these alerts would be skipped.\n",
    "\n",
    "This is why you must explicitly call `stream.commit()` once you're certain the received alerts were successfully processed.\n",
    "\n",
    "Now go back up, uncomment the `stream.commit()` lines, and re-execute these two cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's verify nothing was lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(alerts) == set(alerts1 + alerts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Kafka delivery guarantees\n",
    "\n",
    "A couple of warnings about Kafka's delivery guarantees:\n",
    "* **Kafka does not guarantee the order in which you'll receive the alerts**: it guarantees delivery, but some may be out of order.\n",
    "* **Kafka (typically) guarantees \"at least once\" delivery**: that is, you may receive some alerts more than once (if there's a crash, a network interruption, or any similar exceptional situation). Your code should guard agaist this. Exactly-once semantics difficult, but possible (and coming in the future).\n",
    "\n",
    "![Distributed computing problems](https://cdn.confluent.io/wp-content/uploads/image2.png)\n",
    "\n",
    "-- [Mathias Verraes](https://www.linkedin.com/in/mathiasverraes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the stream\n",
    "\n",
    "Now let's filter the stream for objects of interest. Say we're only interested in asteroids, and wish to ignore the rest.\n",
    "\n",
    "We'll write a filter function which checks whether the alert candidate has the \"Nearby Solar System object Name\" field set to something other than \"null\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_asteroids(alert):\n",
    "    if alert['candidate']['ssnamenr'] != 'null':\n",
    "        return alert\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(\"kafka://genesis.alerts.wtf/medium\", start_at=\"earliest\") as stream:\n",
    "    for idx, alert in stream(limit=10, timeout=10, progress=True, filter=filter_asteroids):\n",
    "        print(f\"[{idx}] Candidate ID: {alert['candid']} {alert['candidate']['ssnamenr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the [fields available in the alert packet](https://zwickytransientfacility.github.io/ztf-avro-alert/schema.html), you can construct arbitrarily complex filters.\n",
    "\n",
    "Here's one that checks whether an object may be a transient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "def is_transient(alert):\n",
    "    # Filter by E. C. Bellm (@ebellm on GitHub)\n",
    "\n",
    "    # if only a single discovery, bail out -- we wait for at least two\n",
    "    # before triggering\n",
    "    if alert['prv_candidates'] is None:\n",
    "        return\n",
    "    \n",
    "    dflc = pd.DataFrame( [ alert['candidate'] ] + alert['prv_candidates'])\n",
    "    candidate = dflc.loc[0]\n",
    "\n",
    "    # positive subtraction?\n",
    "    is_positive_sub = candidate['isdiffpos'] == 't'\n",
    "    \n",
    "    # no nearby source\n",
    "    if (candidate['distpsnr1'] is None) or (candidate['distpsnr1'] > 1.5):\n",
    "        no_pointsource_counterpart = True\n",
    "    else:\n",
    "        # nearby source, but it's a galaxy?\n",
    "        if candidate['sgscore1'] < 0.5:\n",
    "            no_pointsource_counterpart = True\n",
    "        else:\n",
    "            no_pointsource_counterpart = False\n",
    "            \n",
    "    where_detected = (dflc['isdiffpos'] == 't') # nondetections will be None\n",
    "    if np.sum(where_detected) >= 2:\n",
    "        detection_times = dflc.loc[where_detected,'jd'].values\n",
    "        dt = np.diff(detection_times)\n",
    "        not_moving = np.max(dt) >= (30*u.minute).to(u.day).value\n",
    "    else:\n",
    "        not_moving = False\n",
    "    \n",
    "    no_ssobject = (candidate['ssdistnr'] is None) or (candidate['ssdistnr'] < 0) or (candidate['ssdistnr'] > 5)\n",
    "    \n",
    "    if is_positive_sub and no_pointsource_counterpart and not_moving and no_ssobject:\n",
    "        return alert\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with gs.open(\"kafka://genesis.alerts.wtf/medium\", start_at=\"earliest\") as stream:\n",
    "    for idx, alert in stream(limit=10, timeout=10, progress=True, filter=is_transient):\n",
    "        print(f\"[{idx}] Candidate ID: {alert['candid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter in the above example was (intentionally) written to be slow. At this processing rate, it may not be able to keep up with the full LSST alert stream.\n",
    "\n",
    "Fortunately, Genesis knows how to parallelize execution over multiple cores, using Python's `multiprocessing.Pool`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool(4) as workers:\n",
    "    with gs.open(\"kafka://genesis.alerts.wtf/medium\", start_at=\"earliest\") as stream:\n",
    "        for idx, alert in stream(pool=workers, limit=10, timeout=10, progress=True, filter=is_transient):\n",
    "            print(f\"[{idx}] Candidate ID: {alert['candid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness\n",
    "\n",
    "Kafka is a distributed system robust to component failures.\n",
    "\n",
    "The cluster of kafka brokers in our demo setup has a \"replication factor\" of 2 -- that is, each alert is mirrored on at least two brokers. Therefore, one broker may fail without a data loss; the clients will transparently switch to receive data from the other replica. Let's demonstrate this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I give a signal, please start the cell below. It will start downloading alerts from a topic with 20,000 alerts. As it's running, I will shut down one of the brokers in the cluster; your client should still download the full 20,000 alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.open(\"kafka://genesis.alerts.wtf/medium\", start_at=\"earliest\") as stream:\n",
    "    for idx, alert in stream(timeout=10, progress=True):\n",
    "        # we'll do nothing -- just show the progress bar.\n",
    "        pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability\n",
    "\n",
    "And now for the main event: let's see if we can stream and filter alerts at the full LSST rate!\n",
    "\n",
    "I have set up a script that injects 10,000 LSST-sized ZTF alerts every 40 seconds (average LSST rate).\n",
    "\n",
    "When I give the signal, please execute the cell below to start consuming from this stream. We will observe how many simultaneous users we can have before the system fails to keep up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gs.Pool(4) as workers:\n",
    "    with gs.open(\"kafka://genesis.alerts.wtf/lsst\") as stream:\n",
    "        for idx, alert in stream(pool=workers, progress=True):\n",
    "            # we'll do nothing -- just show the progress bar.\n",
    "            pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this how you do it?\n",
    "\n",
    "The idea at this stage was to demonstrate that sending the full stream of LSST alerts to end-users via a cloud resource is possible and cost effective.\n",
    "\n",
    "The cost of machinery for this demo:\n",
    "* Operations cost: **\\$640/month** for the broker cluster\n",
    "* End-user cost: **\\$40/month** (for the analysis machine)\n",
    "\n",
    "The final infrastructure costs may be ~2-3x larger, but still small.\n",
    "\n",
    "This is a starting point to build on. For example:\n",
    "* Integration with services like [AWS Lambda](https://aws.amazon.com/lambda/)\n",
    "* Stream transformation with [KSQL](https://www.confluent.io/product/ksql/)\n",
    "* Integration with [Apache Spark](http://spark.apache.org/) for scalable unified analytics.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This work has been made possible and supported by\n",
    "\n",
    "![Supported By](figures/foundation-logos.png)\n",
    "![Also By](https://www.lsst.org/sites/default/files/Funding-logos-bk.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
